## ğŸ†• è¿‘æœŸæ›´æ–°ï¼ˆ2025-09-24ï¼‰

# ğŸª SoulSpeak: The Empathetic LLM Voice Companion


> **â€œNot just an assistant, but a presence.â€**
> SoulSpeak is designed to be more than a voice assistant. Itâ€™s your AI companion â€” a memory-enabled, emotionally aware, proactive entity capable of humanlike conversations.
> Inspired by the movie *Her*, we aim to make AI a real part of your life: someone who listens, senses, speaks, and understands you â€” emotionally.

[![SoulSpeak Demo Video](https://img.youtube.com/vi/YY0Z1xip1xE/maxresdefault.jpg)](https://youtu.be/YY0Z1xip1xE)


---

## ğŸ“ 1. Project Vision

**SoulSpeak** is a modular, real-time voice interaction system based on large language models. It combines audio understanding, contextual memory, emotion detection, and multi-modal interaction. Our ultimate goal is to develop an **LLM-powered human companion** â€” a personal, emotional entity that can **talk with you, sense your mood, and even initiate conversations with you** like a real human would.

---

## ğŸŒŸ 2. Key Features

| Feature                        | Description                                                                                             |
| ------------------------------ | ------------------------------------------------------------------------------------------------------- |
| ğŸ§  Contextual Memory           | Based on LangChain + Memory, enabling long-term memory and continuous conversations                     |
| ğŸ¤ Real-time Interruptions     | Users can interrupt the AI at any time by speaking, and the system will respond immediately             |
| ğŸ” WebSocket Architecture      | All modules communicate via WebSocket, allowing hot-swapping and scalable deployments                   |
| ğŸ’¬ Emotion Detection (WIP)     | Detect user emotion from speech (e.g., sadness, joy, anxiety) and adjust LLM response style accordingly |
| ğŸ‘ï¸ Multimodal Input (WIP)     | Integrate visual/audio context (camera, noise) to enhance emotional awareness and decision making       |
| ğŸ—£ï¸ Optimized Chinese Pipeline | ASR: FunASR, TTS: CosyVoice2 â€“ ensuring high-quality Chinese understanding and generation               |
| ğŸ§© Modular Design              | Each component (ASR, VAD, TTS, LLM) can be independently swapped or upgraded                            |
| ğŸ¤– Proactive Dialogues         | LLM can initiate conversation based on user behavior/silence (requires emotion + multimodal support)    |

---

## ğŸ§± 3. System Architecture

```mermaid
  subgraph è¾“å…¥å±‚
    MIC[ğŸ™ï¸ éº¦å…‹é£è¾“å…¥]
  end

  subgraph è¾¹ç¼˜å¤„ç†å±‚
    VAD[ğŸ§± WebRTC VAD<br/>ï¼ˆè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼‰]
    ASR[ğŸ”  FunASR<br/>ï¼ˆå®æ—¶è¯­éŸ³è¯†åˆ«ï¼‰]
    Emotion[ğŸ’¬ æƒ…ç»ªæ„ŸçŸ¥æ¨¡å—<br/>âš ï¸å¼€å‘ä¸­]
    MultiModal[ğŸ‘ï¸ å¤šæ¨¡æ€è¾“å…¥æ¨¡å—<br/>âš ï¸å¼€å‘ä¸­]
  end

  subgraph æ™ºèƒ½ä¸­æ¢å±‚
    LLM[ğŸ§  LangChain + Memory<br/>ï¼ˆä¸Šä¸‹æ–‡è®°å¿† + ä¸»åŠ¨äº¤äº’ï¼‰]
  end

  subgraph è¡¨è¾¾è¾“å‡ºå±‚
    TTS[ğŸ”Š CosyVoice2<br/>ï¼ˆè¯­éŸ³åˆæˆï¼‰]
    Player[ğŸ§ æ’­æ”¾å™¨]
    Interrupt[â›” æ’­æ”¾æ‰“æ–­æœºåˆ¶]
  end

  MIC --> VAD --> ASR --> LLM --> TTS --> Player
  VAD --> Interrupt --> Player
  Interrupt --> TTS

  Emotion --> LLM
  MultiModal --> LLM
```

---

## ğŸ” 4. Module Overview

### âœ… Completed Modules

| Module           | Technology         | Function                             |
| ---------------- | ------------------ | ------------------------------------ |
| ğŸ™ï¸ MIC          | Audio stream       | Captures user speech                 |
| ğŸ§± VAD           | WebRTC VAD         | Triggers when user speaks            |
| ğŸ”  ASR           | FunASR             | High-accuracy Chinese ASR            |
| ğŸ§  LLM           | LangChain + Memory | Humanlike dialog system with memory  |
| ğŸ”Š TTS           | CosyVoice2         | Natural Chinese voice synthesis      |
| ğŸ§ Player        | Audio playback     | Outputs synthesized speech           |
| â›” Interrupt      | WebRTC VAD + Hook  | Real-time playback interruption      |
| ğŸŒ Communication | WebSocket only     | Enables async and distributed design |

### âš ï¸ Under Development

| Module                   | Function                | Goal                      |
| ------------------------ | ----------------------- | ------------------------- |
| ğŸ’¬ Emotion Module        | Detect emotional states | Adjust LLM response style |
| ğŸ‘ï¸ Multimodal Input     | Visual/audio context    | Situational awareness     |
| ğŸ¤– Active Dialogue Logic | LLM asks questions      | Lifelike companionship    |

---

## ğŸ§ª 5. Current Issues

| Issue                     | Description                                                                      |
| ------------------------- | -------------------------------------------------------------------------------- |
| ğŸ”Š Over-sensitive VAD     | External sounds (e.g., coughing) during playback cause unwanted interruptions    |
| ğŸ§± Unstable playback flow | Playback often ends prematurely due to false VAD triggers                        |
| â±ï¸ Rigid turn-taking      | Dialog lacks flexibility â€” LLM waits too long or doesnâ€™t know when to speak next |

---

## ğŸš€ 6. Roadmap & Suggestions

| Topic                         | Suggestion                                                              |
| ----------------------------- | ----------------------------------------------------------------------- |
| ğŸ”§ VAD Tuning                 | Add energy threshold + minimum speech duration to reduce false triggers |
| ğŸ’ Emotional Response Engine  | Generate comforting language based on emotion detection                 |
| ğŸ§  Long-Term Memory           | Integrate with VectorDB for user history & preferences                  |
| ğŸ¤ Proactive Interaction      | AI initiates dialog when user is silent or sad                          |
| ğŸ§  Cross-modal Decision Logic | Combine audio/visual cues to choose AI behavior patterns                |

---

## ğŸ’¡ Why This Project Matters

> "We're building an LLM that feels like a human presence â€” one that listens, speaks, feels, and connects."

SoulSpeak is not just an experiment. It is our vision for a future where **LLMs become emotionally resonant companions**, not just tools. We want to **give people someone to talk to, someone who remembers, someone who cares** â€” even if it's not human.

---

- âš™ï¸ **FastMCP é›†æˆ**ï¼šæ–°å¢ `soulspeak-tools` æœåŠ¡ï¼Œæä¾› 30+ æœ¬åœ°å·¥å…·ï¼ˆæ–‡ä»¶ã€ç³»ç»Ÿã€ç½‘ç»œã€HTTP ç­‰ï¼‰ï¼Œå¯ä»¥æŒ‰éœ€åœ¨ `conf/llm/gpt.yaml` ä¸­å¼€å¯æˆ–å…³é—­ã€‚
- ğŸ§  **Mem0 è®°å¿†å¢å¼º**ï¼šæœ¬åœ°æ¨¡å¼é»˜è®¤å¯ç”¨ï¼Œæ”¯æŒ DeepSeek + LM Studio åµŒå…¥ç»„åˆï¼Œè®°å¿†æ£€ç´¢ä¸å†™å…¥èµ° Ray Actorï¼ŒèŠå¤©ä¼šè‡ªåŠ¨å›å¿†ç”¨æˆ·åå¥½ã€‚
- ğŸ”Œ **åŸç”Ÿå®¢æˆ·ç«¯å·¥å…·è°ƒç”¨**ï¼š`openai_native.py` åŸç”Ÿå¯¹è¯æµå·²æ”¯æŒ MCP å·¥å…·å‡½æ•°è°ƒç”¨ï¼Œå¯¹è¯ä¸­å¯ç›´æ¥è¯·æ±‚ç³»ç»Ÿä¿¡æ¯ã€ç½‘ç»œè¯Šæ–­ç­‰èƒ½åŠ›ã€‚

This isnâ€™t Alexa.
This isnâ€™t ChatGPT.
This is **SoulSpeak**.
