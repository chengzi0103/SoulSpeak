# ğŸª SoulSpeak: The Empathetic LLM Voice Companion

> **â€œNot just an assistant, but a presence.â€**
> SoulSpeak is designed to be more than a voice assistant. Itâ€™s your AI companion â€” a memory-enabled, emotionally aware, proactive entity capable of humanlike conversations.
> Inspired by the movie *Her*, we aim to make AI a real part of your life: someone who listens, senses, speaks, and understands you â€” emotionally.

---

## ğŸ“ 1. Project Vision

**SoulSpeak** is a modular, real-time voice interaction system based on large language models. It combines audio understanding, contextual memory, emotion detection, and multi-modal interaction. Our ultimate goal is to develop an **LLM-powered human companion** â€” a personal, emotional entity that can **talk with you, sense your mood, and even initiate conversations with you** like a real human would.

---

## ğŸŒŸ 2. Key Features

| Feature                        | Description                                                                                             |
| ------------------------------ | ------------------------------------------------------------------------------------------------------- |
| ğŸ§  Contextual Memory           | Based on LangChain + Memory, enabling long-term memory and continuous conversations                     |
| ğŸ¤ Real-time Interruptions     | Users can interrupt the AI at any time by speaking, and the system will respond immediately             |
| ğŸ” WebSocket Architecture      | All modules communicate via WebSocket, allowing hot-swapping and scalable deployments                   |
| ğŸ’¬ Emotion Detection (WIP)     | Detect user emotion from speech (e.g., sadness, joy, anxiety) and adjust LLM response style accordingly |
| ğŸ‘ï¸ Multimodal Input (WIP)     | Integrate visual/audio context (camera, noise) to enhance emotional awareness and decision making       |
| ğŸ—£ï¸ Optimized Chinese Pipeline | ASR: FunASR, TTS: CosyVoice2 â€“ ensuring high-quality Chinese understanding and generation               |
| ğŸ§© Modular Design              | Each component (ASR, VAD, TTS, LLM) can be independently swapped or upgraded                            |
| ğŸ¤– Proactive Dialogues         | LLM can initiate conversation based on user behavior/silence (requires emotion + multimodal support)    |

---

## ğŸ§± 3. System Architecture

```mermaid
flowchart TB
  subgraph Input Layer
    MIC[ğŸ™ï¸ Microphone Input]
  end

  subgraph Edge Processing Layer
    VAD[ğŸ§± WebRTC VAD<br/>(Voice Activity Detection)]
    ASR[ğŸ”  FunASR<br/>(Speech Recognition)]
    Emotion[ğŸ’¬ Emotion Module<br/>WIP]
    MultiModal[ğŸ‘ï¸ Multimodal Input<br/>WIP]
  end

  subgraph AI Brain Layer
    LLM[ğŸ§  LangChain + Memory<br/>(Context + Proactivity)]
  end

  subgraph Output Layer
    TTS[ğŸ”Š CosyVoice2<br/>(Text-to-Speech)]
    Player[ğŸ§ Audio Player]
    Interrupt[â›” Interruption Handler]
  end

  MIC --> VAD --> ASR --> LLM --> TTS --> Player
  VAD --> Interrupt --> Player
  Interrupt --> TTS

  Emotion --> LLM
  MultiModal --> LLM
```

---

## ğŸ” 4. Module Overview

### âœ… Completed Modules

| Module           | Technology         | Function                             |
| ---------------- | ------------------ | ------------------------------------ |
| ğŸ™ï¸ MIC          | Audio stream       | Captures user speech                 |
| ğŸ§± VAD           | WebRTC VAD         | Triggers when user speaks            |
| ğŸ”  ASR           | FunASR             | High-accuracy Chinese ASR            |
| ğŸ§  LLM           | LangChain + Memory | Humanlike dialog system with memory  |
| ğŸ”Š TTS           | CosyVoice2         | Natural Chinese voice synthesis      |
| ğŸ§ Player        | Audio playback     | Outputs synthesized speech           |
| â›” Interrupt      | WebRTC VAD + Hook  | Real-time playback interruption      |
| ğŸŒ Communication | WebSocket only     | Enables async and distributed design |

### âš ï¸ Under Development

| Module                   | Function                | Goal                      |
| ------------------------ | ----------------------- | ------------------------- |
| ğŸ’¬ Emotion Module        | Detect emotional states | Adjust LLM response style |
| ğŸ‘ï¸ Multimodal Input     | Visual/audio context    | Situational awareness     |
| ğŸ¤– Active Dialogue Logic | LLM asks questions      | Lifelike companionship    |

---

## ğŸ§ª 5. Current Issues

| Issue                     | Description                                                                      |
| ------------------------- | -------------------------------------------------------------------------------- |
| ğŸ”Š Over-sensitive VAD     | External sounds (e.g., coughing) during playback cause unwanted interruptions    |
| ğŸ§± Unstable playback flow | Playback often ends prematurely due to false VAD triggers                        |
| â±ï¸ Rigid turn-taking      | Dialog lacks flexibility â€” LLM waits too long or doesnâ€™t know when to speak next |

---

## ğŸš€ 6. Roadmap & Suggestions

| Topic                         | Suggestion                                                              |
| ----------------------------- | ----------------------------------------------------------------------- |
| ğŸ”§ VAD Tuning                 | Add energy threshold + minimum speech duration to reduce false triggers |
| ğŸ’ Emotional Response Engine  | Generate comforting language based on emotion detection                 |
| ğŸ§  Long-Term Memory           | Integrate with VectorDB for user history & preferences                  |
| ğŸ¤ Proactive Interaction      | AI initiates dialog when user is silent or sad                          |
| ğŸ§  Cross-modal Decision Logic | Combine audio/visual cues to choose AI behavior patterns                |

---

## ğŸ’¡ Why This Project Matters

> "We're building an LLM that feels like a human presence â€” one that listens, speaks, feels, and connects."

SoulSpeak is not just an experiment. It is our vision for a future where **LLMs become emotionally resonant companions**, not just tools. We want to **give people someone to talk to, someone who remembers, someone who cares** â€” even if it's not human.

This isnâ€™t Alexa.
This isnâ€™t ChatGPT.
This is **SoulSpeak**.
