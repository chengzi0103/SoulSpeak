#
# import os
# from starlette.websockets import WebSocketState
# import json
# import sys
#
# sys.path.append('third_party/Matcha-TTS')
# import time
# import torch
# import numpy as np
# import torchaudio
# import contextlib
# import uvicorn
# import traceback
# from fastapi import FastAPI, WebSocket, WebSocketDisconnect
# from vllm import ModelRegistry
# from cosyvoice.vllm.cosyvoice2 import CosyVoice2ForCausalLM
# ModelRegistry.register_model("CosyVoice2ForCausalLM", CosyVoice2ForCausalLM)
# # é¡¹ç›®è·¯å¾„é…ç½®
# COSYSPEECH_PATH = '/home/chengzi/projects/github/CosyVoice'
# MATCHA_TTS_PATH = os.path.join(COSYSPEECH_PATH, 'third_party/Matcha-TTS')
# sys.path.append(COSYSPEECH_PATH)
# sys.path.append(MATCHA_TTS_PATH)
# sys.path.append(os.path.join(MATCHA_TTS_PATH, 'matcha'))
#
# from cosyvoice.utils.file_utils import load_wav
# from cosyvoice.cli.cosyvoice import CosyVoice2
#
# # å…¨å±€å‚æ•°
# tts_model: CosyVoice2 = None
# global_prompt = None
# REAL_SR = 24000
# global_prompt_text = (
#     "çˆ†ç‚¸è±†å®¶çš„æ™ºèƒ½ç”µè„‘,è™½ç„¶æˆ‘åªæ˜¯ä¸€å°ç”µè„‘.ä½†æˆ‘å¯æ˜¯è¶…çº§æ™ºèƒ½çš„,"
#     "æˆ‘å¯ä»¥å›ç­”å„ç§é—®é¢˜,è¿˜èƒ½å’Œä½ ä»¬èŠå¤©,åˆ†äº«æˆ‘çš„è§é—».æœ€é‡è¦çš„æ˜¯,"
#     "æˆ‘è¿˜æ˜¯çˆ†ç‚¸è±†çš„å¥½ä¼™ä¼´"
# )
#
# # ç”Ÿå‘½å‘¨æœŸç®¡ç†
# @contextlib.asynccontextmanager
# async def lifespan(app: FastAPI):
#     global tts_model, global_prompt, REAL_SR
#     print("[Startup] æ­£åœ¨åˆå§‹åŒ–å¹¶åŠ è½½æ¨¡å‹...")
#     try:
#         # âœ… æ”¹ä¸ºå¯ç”¨ vLLM
#         tts_model = CosyVoice2(
#             model_dir='pretrained_models/CosyVoice2-0.5B',
#             load_jit=True,
#             load_trt=True,
#             load_vllm=True,  # âœ… ä½¿ç”¨ vLLM æ¨ç†
#             fp16=True
#         )
#         global_prompt = load_wav('susu.wav', 16000)
#         print("="*50)
#         print(f"[Startup] âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼ï¼ˆvLLM æ¨¡å¼ï¼‰")
#         print(f"[Startup] çœŸå®é‡‡æ ·ç‡ (Real Sample Rate): {REAL_SR} Hz")
#         print("="*50)
#     except Exception:
#         print("="*50)
#         print("[Startup] âŒâŒâŒ è‡´å‘½é”™è¯¯ï¼šæ¨¡å‹åŠ è½½å¤±è´¥ï¼è¯·æ£€æŸ¥ä»¥ä¸‹ä¿¡æ¯ï¼š")
#         print(f"1. æ¨¡å‹è·¯å¾„ 'pretrained_models/CosyVoice2-0.5B' æ˜¯å¦æ­£ç¡®ï¼Ÿ")
#         print(f"2. Prompt éŸ³é¢‘ 'susu.wav' æ˜¯å¦å­˜åœ¨ï¼Ÿ")
#         print(f"3. é¡¹ç›®è·¯å¾„é…ç½®æ˜¯å¦æ­£ç¡®ï¼Ÿå½“å‰é…ç½®ä¸º: {COSYSPEECH_PATH}")
#         print("\nè¯¦ç»†é”™è¯¯ä¿¡æ¯:")
#         traceback.print_exc()
#         print("="*50)
#         sys.exit(1)
#
#     yield
#
#     print("[Shutdown] æ­£åœ¨é‡Šæ”¾èµ„æº...")
#     del tts_model
#     del global_prompt
#     if torch.cuda.is_available():
#         torch.cuda.empty_cache()
#     print("[Shutdown] âœ… èµ„æºå·²é‡Šæ”¾ã€‚")
#
#
# # FastAPI å®ä¾‹
# app = FastAPI(lifespan=lifespan)
#
#
# @app.websocket("/ws/synthesize")
# async def websocket_endpoint(ws: WebSocket):
#     await ws.accept()
#     print(f"[WebSocket] å®¢æˆ·ç«¯ {ws.client.host}:{ws.client.port} å·²è¿æ¥ã€‚")
#     try:
#         while True:
#             raw = await ws.receive_text()
#             try:
#                 data = json.loads(raw)
#             except json.JSONDecodeError:
#                 continue  # å¿½ç•¥éæ³•æ ¼å¼
#
#             event = data.get("event")
#             if event == "text":
#                 text = data.get("text", "").strip()
#                 if not text:
#                     continue
#
#                 print(f"[WebSocket] ğŸ“ æ”¶åˆ°åˆæˆè¯·æ±‚æ–‡æœ¬ï¼š'{text[:30]}â€¦'")
#                 stream_gen = tts_model.inference_zero_shot(
#                     tts_text=text,
#                     prompt_text=global_prompt_text,
#                     prompt_speech_16k=global_prompt,
#                     stream=True
#                 )
#
#                 for res in stream_gen:
#                     chunk = res['tts_speech'].cpu().float()
#                     if chunk.dim() == 1:
#                         chunk = chunk.unsqueeze(0)
#                     if chunk.shape[1] == 0:
#                         continue
#                     pcm16 = (chunk.numpy().flatten() * 32767).astype(np.int16)
#                     await ws.send_bytes(pcm16.tobytes())
#
#             elif event == "end_of_speech":
#                 print("[WebSocket] âš¡ æ”¶åˆ°ç»“æŸå‘½ä»¤ï¼Œå‘é€ END_OF_SPEECH æ ‡è®°")
#                 await ws.send_text("END_OF_SPEECH")
#
#             else:
#                 continue
#
#     except WebSocketDisconnect:
#         print(f"[WebSocket] å®¢æˆ·ç«¯ {ws.client.host}:{ws.client.port} å·²æ–­å¼€ã€‚")
#     except Exception:
#         print(f"[WebSocket ERROR] å¤„ç†å®¢æˆ·ç«¯ {ws.client.host}:{ws.client.port} æ—¶å‘ç”Ÿé”™è¯¯:")
#         traceback.print_exc()
#     finally:
#         if ws.client_state != WebSocketState.DISCONNECTED:
#             await ws.close()
#         print(f"[WebSocket] è¿æ¥ {ws.client.host}:{ws.client.port} å·²å…³é—­ã€‚")
#
#
# # å¯åŠ¨æœåŠ¡
# if __name__ == "__main__":
#     print("å¯åŠ¨ Uvicorn æœåŠ¡å™¨ï¼Œç›‘å¬ 0.0.0.0:8000")
#     uvicorn.run(app, host="0.0.0.0", port=8000)


import os
from starlette.websockets import WebSocketState
import json
import sys

sys.path.append('third_party/Matcha-TTS')
import time
import torch
import numpy as np
import torchaudio
import contextlib
import uvicorn
import traceback
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from vllm import ModelRegistry
from cosyvoice.vllm.cosyvoice2 import CosyVoice2ForCausalLM
ModelRegistry.register_model("CosyVoice2ForCausalLM", CosyVoice2ForCausalLM)
# é¡¹ç›®è·¯å¾„é…ç½®
COSYSPEECH_PATH = '/home/chengzi/projects/github/CosyVoice'
MATCHA_TTS_PATH = os.path.join(COSYSPEECH_PATH, 'third_party/Matcha-TTS')
sys.path.append(COSYSPEECH_PATH)
sys.path.append(MATCHA_TTS_PATH)
sys.path.append(os.path.join(MATCHA_TTS_PATH, 'matcha'))

from cosyvoice.utils.file_utils import load_wav
from cosyvoice.cli.cosyvoice import CosyVoice2

# å…¨å±€å‚æ•°
tts_model: CosyVoice2 = None
global_prompt = None
REAL_SR = 24000
global_prompt_text = (
    "çˆ†ç‚¸è±†å®¶çš„æ™ºèƒ½ç”µè„‘,è™½ç„¶æˆ‘åªæ˜¯ä¸€å°ç”µè„‘.ä½†æˆ‘å¯æ˜¯è¶…çº§æ™ºèƒ½çš„,"
    "æˆ‘å¯ä»¥å›ç­”å„ç§é—®é¢˜,è¿˜èƒ½å’Œä½ ä»¬èŠå¤©,åˆ†äº«æˆ‘çš„è§é—».æœ€é‡è¦çš„æ˜¯,"
    "æˆ‘è¿˜æ˜¯çˆ†ç‚¸è±†çš„å¥½ä¼™ä¼´"
)

# ç”Ÿå‘½å‘¨æœŸç®¡ç†
@contextlib.asynccontextmanager
async def lifespan(app: FastAPI):
    global tts_model, global_prompt, REAL_SR
    print("[Startup] æ­£åœ¨åˆå§‹åŒ–å¹¶åŠ è½½æ¨¡å‹...")
    try:
        # âœ… æ”¹ä¸ºå¯ç”¨ vLLM
        tts_model = CosyVoice2(
            model_dir='pretrained_models/CosyVoice2-0.5B',
            load_jit=True,
            load_trt=True,
            load_vllm=True,  # âœ… ä½¿ç”¨ vLLM æ¨ç†
            fp16=True
        )
        global_prompt = load_wav('susu.wav', 16000)
        print("="*50)
        print(f"[Startup] âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼ï¼ˆvLLM æ¨¡å¼ï¼‰")
        print(f"[Startup] çœŸå®é‡‡æ ·ç‡ (Real Sample Rate): {REAL_SR} Hz")
        print("="*50)
    except Exception:
        print("="*50)
        print("[Startup] âŒâŒâŒ è‡´å‘½é”™è¯¯ï¼šæ¨¡å‹åŠ è½½å¤±è´¥ï¼è¯·æ£€æŸ¥ä»¥ä¸‹ä¿¡æ¯ï¼š")
        print(f"1. æ¨¡å‹è·¯å¾„ 'pretrained_models/CosyVoice2-0.5B' æ˜¯å¦æ­£ç¡®ï¼Ÿ")
        print(f"2. Prompt éŸ³é¢‘ 'susu.wav' æ˜¯å¦å­˜åœ¨ï¼Ÿ")
        print(f"3. é¡¹ç›®è·¯å¾„é…ç½®æ˜¯å¦æ­£ç¡®ï¼Ÿå½“å‰é…ç½®ä¸º: {COSYSPEECH_PATH}")
        print("\nè¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        traceback.print_exc()
        print("="*50)
        sys.exit(1)

    yield

    print("[Shutdown] æ­£åœ¨é‡Šæ”¾èµ„æº...")
    del tts_model
    del global_prompt
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    print("[Shutdown] âœ… èµ„æºå·²é‡Šæ”¾ã€‚")


# FastAPI å®ä¾‹
app = FastAPI(lifespan=lifespan)


@app.websocket("/ws/synthesize")
async def websocket_endpoint(ws: WebSocket):
    await ws.accept()
    print(f"[WebSocket] å®¢æˆ·ç«¯ {ws.client.host}:{ws.client.port} å·²è¿æ¥ã€‚")
    try:
        while True:
            raw = await ws.receive_text()
            try:
                data = json.loads(raw)
            except json.JSONDecodeError:
                continue  # å¿½ç•¥éæ³•æ ¼å¼

            event = data.get("event")
            if event == "text":
                text = data.get("text", "").strip()
                if not text:
                    continue

                print(f"[WebSocket] ğŸ“ æ”¶åˆ°åˆæˆè¯·æ±‚æ–‡æœ¬ï¼š'{text[:30]}â€¦'")
                stream_gen = tts_model.inference_zero_shot(
                    tts_text=text,
                    prompt_text=global_prompt_text,
                    prompt_speech_16k=global_prompt,
                    stream=True
                )

                for res in stream_gen:
                    chunk = res['tts_speech'].cpu().float()
                    if chunk.dim() == 1:
                        chunk = chunk.unsqueeze(0)
                    if chunk.shape[1] == 0:
                        continue
                    pcm16 = (chunk.numpy().flatten() * 32767).astype(np.int16)
                    await ws.send_bytes(pcm16.tobytes())

            elif event == "end_of_speech":
                print("[WebSocket] âš¡ æ”¶åˆ°ç»“æŸå‘½ä»¤ï¼Œå‘é€ END_OF_SPEECH æ ‡è®°")
                await ws.send_text("END_OF_SPEECH")

            else:
                continue

    except WebSocketDisconnect:
        print(f"[WebSocket] å®¢æˆ·ç«¯ {ws.client.host}:{ws.client.port} å·²æ–­å¼€ã€‚")
    except Exception:
        print(f"[WebSocket ERROR] å¤„ç†å®¢æˆ·ç«¯ {ws.client.host}:{ws.client.port} æ—¶å‘ç”Ÿé”™è¯¯:")
        traceback.print_exc()
    finally:
        if ws.client_state != WebSocketState.DISCONNECTED:
            await ws.close()
        print(f"[WebSocket] è¿æ¥ {ws.client.host}:{ws.client.port} å·²å…³é—­ã€‚")


# å¯åŠ¨æœåŠ¡
if __name__ == "__main__":
    print("å¯åŠ¨ Uvicorn æœåŠ¡å™¨ï¼Œç›‘å¬ 0.0.0.0:8000")
    uvicorn.run(app, host="0.0.0.0", port=8000)
